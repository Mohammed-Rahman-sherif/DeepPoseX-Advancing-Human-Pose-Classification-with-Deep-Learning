{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkxTF5xgD96L",
        "outputId": "98592ce0-d8d2-467b-e371-edff895ce4ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pafy\n",
            "  Downloading pafy-0.5.5-py2.py3-none-any.whl (35 kB)\n",
            "Collecting youtube-dl\n",
            "  Downloading youtube_dl-2021.12.17-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.23.5)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.1)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.8)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2023.7.22)\n",
            "Installing collected packages: youtube-dl, pafy\n",
            "Successfully installed pafy-0.5.5 youtube-dl-2021.12.17\n"
          ]
        }
      ],
      "source": [
        "# Install the required libraries.\n",
        "!pip install pafy youtube-dl moviepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwAuVC9KoVwg"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries.\n",
        "import os\n",
        "import cv2\n",
        "import pafy\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from moviepy.editor import *\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Import the required libraries.\n",
        "import os\n",
        "import cv2\n",
        "import pafy\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from moviepy.editor import *\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0HPaME_ofrG"
      },
      "outputs": [],
      "source": [
        "seed_constant = 27\n",
        "np.random.seed(seed_constant)\n",
        "random.seed(seed_constant)\n",
        "tf.random.set_seed(seed_constant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzBz9NaDonT-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Downlaod the UCF50 Dataset\n",
        "!wget --no-check-certificate https://www.crcv.ucf.edu/data/UCF50.rar\n",
        "\n",
        "#Extract the Dataset\n",
        "!unrar x UCF50.rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1Y3IkGC6otgn",
        "outputId": "c7755c8c-d45c-4fb2-b9cb-56e8ea1a571b"
      },
      "outputs": [],
      "source": [
        "# Create a Matplotlib figure and specify the size of the figure.\n",
        "plt.figure(figsize = (20, 20))\n",
        "\n",
        "# Get the names of all classes/categories in UCF50.\n",
        "all_classes_names = os.listdir('UCF50')\n",
        "\n",
        "# Generate a list of 20 random values. The values will be between 0-50,\n",
        "# where 50 is the total number of class in the dataset.\n",
        "random_range = random.sample(range(len(all_classes_names)), 20)\n",
        "\n",
        "# Iterating through all the generated random values.\n",
        "for counter, random_index in enumerate(random_range, 1):\n",
        "\n",
        "    # Retrieve a Class Name using the Random Index.\n",
        "    selected_class_Name = all_classes_names[random_index]\n",
        "\n",
        "    # Retrieve the list of all the video files present in the randomly selected Class Directory.\n",
        "    video_files_names_list = os.listdir(f'UCF50/{selected_class_Name}')\n",
        "\n",
        "    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.\n",
        "    selected_video_file_name = random.choice(video_files_names_list)\n",
        "\n",
        "    # Initialize a VideoCapture object to read from the video File.\n",
        "    video_reader = cv2.VideoCapture(f'UCF50/{selected_class_Name}/{selected_video_file_name}')\n",
        "\n",
        "    # Read the first frame of the video file.\n",
        "    _, bgr_frame = video_reader.read()\n",
        "\n",
        "    # Release the VideoCapture object.\n",
        "    video_reader.release()\n",
        "\n",
        "    # Convert the frame from BGR into RGB format.\n",
        "    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Write the class name on the video frame.\n",
        "    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "\n",
        "    # Display the frame.\n",
        "    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJIbspO-pRON"
      },
      "outputs": [],
      "source": [
        "img_height,img_width = 64,64\n",
        "#specifiying the number of frames fed into the model\n",
        "SEQUENCE_LENGTH = 20\n",
        "# Specify the directory containing the UCF50 dataset.\n",
        "DATASET_DIR = \"UCF50\"\n",
        "# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.\n",
        "CLASSES_LIST = [\"WalkingWithDog\", \"TaiChi\", \"Swing\", \"HorseRace\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmmC9bsQpaf1"
      },
      "outputs": [],
      "source": [
        "def frames_extraction(video_path):\n",
        "    '''\n",
        "    This function will extract the required frames from a video after resizing and normalizing them.\n",
        "    Args:\n",
        "        video_path: The path of the video in the disk, whose frames are to be extracted.\n",
        "    Returns:\n",
        "        frames_list: A list containing the resized and normalized frames of the video.\n",
        "    '''\n",
        "\n",
        "    # Declare a list to store video frames.\n",
        "    frames_list = []\n",
        "\n",
        "    # Read the Video File using the VideoCapture object.\n",
        "    video_reader = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get the total number of frames in the video.\n",
        "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calculate the the interval after which frames will be added to the list.\n",
        "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
        "\n",
        "    # Iterate through the Video Frames.\n",
        "    for frame_counter in range(SEQUENCE_LENGTH):\n",
        "\n",
        "        # Set the current frame position of the video.\n",
        "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
        "\n",
        "        # Reading the frame from the video.\n",
        "        success, frame = video_reader.read()\n",
        "\n",
        "        # Check if Video frame is not successfully read then break the loop\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        # Resize the Frame to fixed height and width.\n",
        "        resized_frame = cv2.resize(frame, (img_height,img_width))\n",
        "\n",
        "        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n",
        "        normalized_frame = resized_frame / 255\n",
        "\n",
        "        # Append the normalized frame into the frames list\n",
        "        frames_list.append(normalized_frame)\n",
        "\n",
        "    # Release the VideoCapture object.\n",
        "    video_reader.release()\n",
        "\n",
        "    # Return the frames list.\n",
        "    return frames_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGLsa_9DptTl"
      },
      "outputs": [],
      "source": [
        "def create_dataset():\n",
        "    '''\n",
        "    This function will extract the data of the selected classes and create the required dataset.\n",
        "    Returns:\n",
        "        features:          A list containing the extracted frames of the videos.\n",
        "        labels:            A list containing the indexes of the classes associated with the videos.\n",
        "        video_files_paths: A list containing the paths of the videos in the disk.\n",
        "    '''\n",
        "\n",
        "    # Declared Empty Lists to store the features, labels and video file path values.\n",
        "    features = []\n",
        "    labels = []\n",
        "    video_files_paths = []\n",
        "\n",
        "    # Iterating through all the classes mentioned in the classes list\n",
        "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
        "\n",
        "        # Display the name of the class whose data is being extracted.\n",
        "        print(f'Extracting Data of Class: {class_name}')\n",
        "\n",
        "        # Get the list of video files present in the specific class name directory.\n",
        "        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n",
        "\n",
        "        # Iterate through all the files present in the files list.\n",
        "        for file_name in files_list:\n",
        "\n",
        "            # Get the complete video path.\n",
        "            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n",
        "\n",
        "            # Extract the frames of the video file.\n",
        "            frames = frames_extraction(video_file_path)\n",
        "\n",
        "            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.\n",
        "            # So ignore the vides having frames less than the SEQUENCE_LENGTH.\n",
        "            if len(frames) == SEQUENCE_LENGTH:\n",
        "\n",
        "                # Append the data to their repective lists.\n",
        "                features.append(frames)\n",
        "                labels.append(class_index)\n",
        "                video_files_paths.append(video_file_path)\n",
        "\n",
        "    # Converting the list to numpy arrays\n",
        "    features = np.asarray(features)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Return the frames, class index, and video file path.\n",
        "    return features, labels, video_files_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oe4YQSVp5SF",
        "outputId": "4e38baba-40be-4034-9d70-443301e8f93f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting Data of Class: WalkingWithDog\n",
            "Extracting Data of Class: TaiChi\n",
            "Extracting Data of Class: Swing\n",
            "Extracting Data of Class: HorseRace\n"
          ]
        }
      ],
      "source": [
        "# Create the dataset.\n",
        "features, labels, video_files_paths = create_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EiaMytOer6s",
        "outputId": "4ea4397c-2adf-42ce-863d-4fc2c8f1893e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(487,)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiIkdFx4p9NV"
      },
      "outputs": [],
      "source": [
        "# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors\n",
        "one_hot_encoded_labels = to_categorical(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxIcj9TDqOKO"
      },
      "outputs": [],
      "source": [
        "# Split the Data into Train ( 75% ) and Test Set ( 25% ).\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.25, shuffle = True, random_state = seed_constant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8x_dYhs-VWd",
        "outputId": "e4b63579-5778-45a2-f7a0-2b453e44fcf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(365, 20, 64, 64, 3)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUXXoexW-1DU",
        "outputId": "384fa703-ebbb-4dfb-94b4-0ab565b89940"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(122, 20, 64, 64, 3)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRDwhss0eal2",
        "outputId": "4b2a70d6-c7d5-4ea8-88c2-d7f1e683cd98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(365, 4)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSUV9aYvqb8O"
      },
      "source": [
        "**Configure the hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eIa-ZdIqTCl"
      },
      "outputs": [],
      "source": [
        "# DATA\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "INPUT_SHAPE = (20, 64, 64, 3)\n",
        "NUM_CLASSES = 4\n",
        "\n",
        "# OPTIMIZER\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "\n",
        "# TRAINING\n",
        "EPOCHS = 60\n",
        "\n",
        "# TUBELET EMBEDDING\n",
        "PATCH_SIZE = (8, 8, 8)\n",
        "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
        "\n",
        "# ViViT ARCHITECTURE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "PROJECTION_DIM = 128\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N1dJS2IqwW2"
      },
      "source": [
        "**Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdukzyUNxhdF",
        "outputId": "c68e98f0-b0f7-4bf2-aae6-5bfceb139234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/612.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/612.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m604.2/612.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTf1qjr3xVSk",
        "outputId": "1680cfd2-b976-4b57-c6cb-b098cbb3d3da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "o2bM04STqpI5",
        "outputId": "36bcf1c0-b14b-4647-8f8e-d2fa5092109f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6eb84afaa492>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     [\n\u001b[1;32m      3\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"horizontal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomRotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'image_size' is not defined"
          ]
        }
      ],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(features_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuInecHvzW9i"
      },
      "source": [
        "**Implement Patch Creation as layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-eRc5HezTqs"
      },
      "outputs": [],
      "source": [
        "class TubeletEmbedding(layers.Layer):\n",
        "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.projection = layers.Conv3D(\n",
        "            filters=embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            strides=patch_size,\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
        "\n",
        "    def call(self, videos):\n",
        "        projected_patches = self.projection(videos)\n",
        "        flattened_patches = self.flatten(projected_patches)\n",
        "        return flattened_patches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSSl24OgwNE1"
      },
      "source": [
        "**Implementing patch encoding layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icB3cvRK3iPk"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_tokens, _ = input_shape\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_tokens, output_dim=self.embed_dim\n",
        "        )\n",
        "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
        "\n",
        "    def call(self, encoded_tokens):\n",
        "        # Encode the positions and add it to the encoded tokens\n",
        "        encoded_positions = self.position_embedding(self.positions)\n",
        "        encoded_tokens = encoded_tokens + encoded_positions\n",
        "        return encoded_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAfbX3La8F4g"
      },
      "outputs": [],
      "source": [
        "def create_vivit_classifier(\n",
        "    tubelet_embedder,\n",
        "    positional_encoder,\n",
        "    input_shape=INPUT_SHAPE,\n",
        "    transformer_layers=NUM_LAYERS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    embed_dim=PROJECTION_DIM,\n",
        "    layer_norm_eps=LAYER_NORM_EPS,\n",
        "    num_classes=NUM_CLASSES,\n",
        "):\n",
        "    # Get the input layer\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Create patches.\n",
        "    patches = tubelet_embedder(inputs)\n",
        "    # Encode patches.\n",
        "    encoded_patches = positional_encoder(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization and MHSA\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.2\n",
        "        )(x1, x1)\n",
        "\n",
        "\n",
        "        # Skip connection\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        # Layer Normalization and MLP\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        #Try covolution instead MLP\n",
        "\n",
        "        x3 = layers.Conv1D(filters=128, kernel_size=1, activation=\"relu\")(x3)\n",
        "        x3 = layers.Dropout(0.1)(x3)\n",
        "        x3 = layers.Conv1D(filters=128, kernel_size=1, activation=\"relu\")(x3)\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x3)\n",
        "        \"\"\"x3 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
        "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
        "            ]\n",
        "        )(x3)\"\"\"\n",
        "\n",
        "        # Skip connection\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "        encoded_patches = layers.Dropout(0.2)(encoded_patches)\n",
        "\n",
        "    # Layer normalization and Global average pooling.\n",
        "    #representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
        "    #representation = layers.GlobalAvgPool1D()(representation)\n",
        "\n",
        "    # Classify outputs.\n",
        "    #outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=encoded_patches)\n",
        "    model.summary()\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7DdF5FGV0g4",
        "outputId": "aa5f62e1-2912-419d-8490-a3f4cf704d0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 20, 64, 64,  0           []                               \n",
            "                                 3)]                                                              \n",
            "                                                                                                  \n",
            " tubelet_embedding_3 (TubeletEm  (None, 128, 128)    196736      ['input_4[0][0]']                \n",
            " bedding)                                                                                         \n",
            "                                                                                                  \n",
            " positional_encoder_3 (Position  (None, 128, 128)    16384       ['tubelet_embedding_3[0][0]']    \n",
            " alEncoder)                                                                                       \n",
            "                                                                                                  \n",
            " layer_normalization_21 (LayerN  (None, 128, 128)    256         ['positional_encoder_3[0][0]']   \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_10 (Multi  (None, 128, 128)    66048       ['layer_normalization_21[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " add_19 (Add)                   (None, 128, 128)     0           ['multi_head_attention_10[0][0]',\n",
            "                                                                  'positional_encoder_3[0][0]']   \n",
            "                                                                                                  \n",
            " layer_normalization_22 (LayerN  (None, 128, 128)    256         ['add_19[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 128, 128)     16512       ['layer_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 128, 128)     0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 128, 128)     16512       ['dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_23 (LayerN  (None, 128, 128)    256         ['conv1d_4[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_20 (Add)                   (None, 128, 128)     0           ['layer_normalization_23[0][0]', \n",
            "                                                                  'add_19[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 128, 128)     0           ['add_20[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_24 (LayerN  (None, 128, 128)    256         ['dropout_10[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_11 (Multi  (None, 128, 128)    66048       ['layer_normalization_24[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " add_21 (Add)                   (None, 128, 128)     0           ['multi_head_attention_11[0][0]',\n",
            "                                                                  'dropout_10[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_25 (LayerN  (None, 128, 128)    256         ['add_21[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 128, 128)     16512       ['layer_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 128, 128)     0           ['conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 128, 128)     16512       ['dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_26 (LayerN  (None, 128, 128)    256         ['conv1d_6[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_22 (Add)                   (None, 128, 128)     0           ['layer_normalization_26[0][0]', \n",
            "                                                                  'add_21[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 128, 128)     0           ['add_22[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_27 (LayerN  (None, 128, 128)    256         ['dropout_12[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_12 (Multi  (None, 128, 128)    66048       ['layer_normalization_27[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " add_23 (Add)                   (None, 128, 128)     0           ['multi_head_attention_12[0][0]',\n",
            "                                                                  'dropout_12[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_28 (LayerN  (None, 128, 128)    256         ['add_23[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 128, 128)     16512       ['layer_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 128, 128)     0           ['conv1d_7[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 128, 128)     16512       ['dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_29 (LayerN  (None, 128, 128)    256         ['conv1d_8[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_24 (Add)                   (None, 128, 128)     0           ['layer_normalization_29[0][0]', \n",
            "                                                                  'add_23[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 128, 128)     0           ['add_24[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_30 (LayerN  (None, 128, 128)    256         ['dropout_14[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_13 (Multi  (None, 128, 128)    66048       ['layer_normalization_30[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " add_25 (Add)                   (None, 128, 128)     0           ['multi_head_attention_13[0][0]',\n",
            "                                                                  'dropout_14[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_31 (LayerN  (None, 128, 128)    256         ['add_25[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 128, 128)     16512       ['layer_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 128, 128)     0           ['conv1d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 128, 128)     16512       ['dropout_15[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_32 (LayerN  (None, 128, 128)    256         ['conv1d_10[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_26 (Add)                   (None, 128, 128)     0           ['layer_normalization_32[0][0]', \n",
            "                                                                  'add_25[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 128, 128)     0           ['add_26[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_33 (LayerN  (None, 128, 128)    256         ['dropout_16[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_14 (Multi  (None, 128, 128)    66048       ['layer_normalization_33[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " add_27 (Add)                   (None, 128, 128)     0           ['multi_head_attention_14[0][0]',\n",
            "                                                                  'dropout_16[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_34 (LayerN  (None, 128, 128)    256         ['add_27[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 128, 128)     0           ['conv1d_11[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 128, 128)     16512       ['dropout_17[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_35 (LayerN  (None, 128, 128)    256         ['conv1d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_28 (Add)                   (None, 128, 128)     0           ['layer_normalization_35[0][0]', \n",
            "                                                                  'add_27[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)           (None, 128, 128)     0           ['add_28[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_36 (LayerN  (None, 128, 128)    256         ['dropout_18[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_15 (Multi  (None, 128, 128)    66048       ['layer_normalization_36[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " add_29 (Add)                   (None, 128, 128)     0           ['multi_head_attention_15[0][0]',\n",
            "                                                                  'dropout_18[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_37 (LayerN  (None, 128, 128)    256         ['add_29[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 128, 128)     0           ['conv1d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 128, 128)     16512       ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_38 (LayerN  (None, 128, 128)    256         ['conv1d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_30 (Add)                   (None, 128, 128)     0           ['layer_normalization_38[0][0]', \n",
            "                                                                  'add_29[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)           (None, 128, 128)     0           ['add_30[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_39 (LayerN  (None, 128, 128)    256         ['dropout_20[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_16 (Multi  (None, 128, 128)    66048       ['layer_normalization_39[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " add_31 (Add)                   (None, 128, 128)     0           ['multi_head_attention_16[0][0]',\n",
            "                                                                  'dropout_20[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_40 (LayerN  (None, 128, 128)    256         ['add_31[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)           (None, 128, 128)     0           ['conv1d_15[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 128, 128)     16512       ['dropout_21[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_41 (LayerN  (None, 128, 128)    256         ['conv1d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_32 (Add)                   (None, 128, 128)     0           ['layer_normalization_41[0][0]', \n",
            "                                                                  'add_31[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)           (None, 128, 128)     0           ['add_32[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_42 (LayerN  (None, 128, 128)    256         ['dropout_22[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_17 (Multi  (None, 128, 128)    66048       ['layer_normalization_42[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " add_33 (Add)                   (None, 128, 128)     0           ['multi_head_attention_17[0][0]',\n",
            "                                                                  'dropout_22[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_43 (LayerN  (None, 128, 128)    256         ['add_33[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)           (None, 128, 128)     0           ['conv1d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 128, 128)     16512       ['dropout_23[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_44 (LayerN  (None, 128, 128)    256         ['conv1d_18[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_34 (Add)                   (None, 128, 128)     0           ['layer_normalization_44[0][0]', \n",
            "                                                                  'add_33[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)           (None, 128, 128)     0           ['add_34[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,011,840\n",
            "Trainable params: 1,011,840\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "trans_model = create_vivit_classifier(\n",
        "        tubelet_embedder=TubeletEmbedding(\n",
        "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
        "        ),\n",
        "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFB2ZBXtGC8z"
      },
      "source": [
        "**Implement Pre-Trained ViT Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lURDVeomGCK9",
        "outputId": "9f0b0027-3b59-4187-cca4-b6a1bf89ea47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting vit_keras\n",
            "  Downloading vit_keras-0.1.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit_keras) (1.10.1)\n",
            "Collecting validators (from vit_keras)\n",
            "  Downloading validators-0.21.2-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->vit_keras) (1.23.5)\n",
            "Installing collected packages: validators, vit_keras\n",
            "Successfully installed validators-0.21.2 vit_keras-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install vit_keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyQN_fiuHIC-",
        "outputId": "6eedd722-e900-45ea-e566-0045b22e77ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 4, 4\n",
            "  warnings.warn(\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from vit_keras import vit\n",
        "\n",
        "vit_model = vit.vit_b16(\n",
        "        image_size = 64,\n",
        "        activation = 'softmax',\n",
        "        pretrained = True,\n",
        "        include_top = False,\n",
        "        pretrained_top = False,\n",
        "        classes = 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GghgFTuJeNG"
      },
      "source": [
        "**Pre-Trained Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQyoC9LhJAhq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import regularizers\n",
        "def build_model():\n",
        "\n",
        "    inputs = layers.Input(shape=(20,64, 64, 3))\n",
        "    \"\"\"for i in range(SEQUENCE_LENGTH):\n",
        "        frame_features = vit_model(inputs[:,i,:,:,:])\n",
        "        features.append(frame_features)\n",
        "    vit_features = tf.stack(features, axis=1)\n",
        "    #set the shape of LSTM\n",
        "    lstm_shape = vit_features.shape\"\"\"\n",
        "\n",
        "    vit_features = create_vivit_classifier(\n",
        "        tubelet_embedder=TubeletEmbedding(\n",
        "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
        "        ),\n",
        "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
        "    )(inputs)\n",
        "    #print(\"shape:\",vit_features.shape)\n",
        "    #(shape: (None, 20, 144, 64))\n",
        "    #data reshape\n",
        "    #vit_features = layers.Reshape((20,144*64))(vit_features)\n",
        "    #print(\"after reshape:\",vit_features.shape)\n",
        "    x1 = Bidirectional(LSTM(512, return_sequences=True, activation='tanh'), input_shape=(128, 128))(vit_features)\n",
        "    x2= Bidirectional(LSTM(512, return_sequences=True,activation='tanh'))(x1)\n",
        "    x3 =Bidirectional(LSTM(512, return_sequences=True,activation='tanh',kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)))(x2)\n",
        "    x4 =Bidirectional(LSTM(512, return_sequences=True,activation='tanh'))(x3)\n",
        "    x5 =Bidirectional(LSTM(512, return_sequences=True,activation='tanh',dropout=0.2))(x4)\n",
        "    x6 =Bidirectional(LSTM(512, return_sequences=True,activation='tanh'))(x5)\n",
        "    x7 =Bidirectional(LSTM(512, return_sequences=True,activation='tanh',kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)))(x6)\n",
        "    x8 =Flatten()(x7)\n",
        "    outputs = Dense(len(CLASSES_LIST), activation = \"softmax\")(x8)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    # display the model summary\n",
        "    model.summary()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRS4TDHETYx7",
        "outputId": "196b07b8-6cb1-4fa1-c1ae-16269f0ab236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_8 (InputLayer)           [(None, 20, 64, 64,  0           []                               \n",
            "                                 3)]                                                              \n",
            "                                                                                                  \n",
            " tubelet_embedding_5 (TubeletEm  (None, 128, 128)    196736      ['input_8[0][0]']                \n",
            " bedding)                                                                                         \n",
            "                                                                                                  \n",
            " positional_encoder_5 (Position  (None, 128, 128)    16384       ['tubelet_embedding_5[0][0]']    \n",
            " alEncoder)                                                                                       \n",
            "                                                                                                  \n",
            " layer_normalization_69 (LayerN  (None, 128, 128)    256         ['positional_encoder_5[0][0]']   \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_26 (Multi  (None, 128, 128)    66048       ['layer_normalization_69[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_69[0][0]'] \n",
            "                                                                                                  \n",
            " add_51 (Add)                   (None, 128, 128)     0           ['multi_head_attention_26[0][0]',\n",
            "                                                                  'positional_encoder_5[0][0]']   \n",
            "                                                                                                  \n",
            " layer_normalization_70 (LayerN  (None, 128, 128)    256         ['add_51[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_35 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_70[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_41 (Dropout)           (None, 128, 128)     0           ['conv1d_35[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_36 (Conv1D)             (None, 128, 128)     16512       ['dropout_41[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_71 (LayerN  (None, 128, 128)    256         ['conv1d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_52 (Add)                   (None, 128, 128)     0           ['layer_normalization_71[0][0]', \n",
            "                                                                  'add_51[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_42 (Dropout)           (None, 128, 128)     0           ['add_52[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_72 (LayerN  (None, 128, 128)    256         ['dropout_42[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_27 (Multi  (None, 128, 128)    66048       ['layer_normalization_72[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_72[0][0]'] \n",
            "                                                                                                  \n",
            " add_53 (Add)                   (None, 128, 128)     0           ['multi_head_attention_27[0][0]',\n",
            "                                                                  'dropout_42[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_73 (LayerN  (None, 128, 128)    256         ['add_53[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_37 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_73[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_43 (Dropout)           (None, 128, 128)     0           ['conv1d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_38 (Conv1D)             (None, 128, 128)     16512       ['dropout_43[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_74 (LayerN  (None, 128, 128)    256         ['conv1d_38[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_54 (Add)                   (None, 128, 128)     0           ['layer_normalization_74[0][0]', \n",
            "                                                                  'add_53[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_44 (Dropout)           (None, 128, 128)     0           ['add_54[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_75 (LayerN  (None, 128, 128)    256         ['dropout_44[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_28 (Multi  (None, 128, 128)    66048       ['layer_normalization_75[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_75[0][0]'] \n",
            "                                                                                                  \n",
            " add_55 (Add)                   (None, 128, 128)     0           ['multi_head_attention_28[0][0]',\n",
            "                                                                  'dropout_44[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_76 (LayerN  (None, 128, 128)    256         ['add_55[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_39 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_76[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_45 (Dropout)           (None, 128, 128)     0           ['conv1d_39[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_40 (Conv1D)             (None, 128, 128)     16512       ['dropout_45[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_77 (LayerN  (None, 128, 128)    256         ['conv1d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_56 (Add)                   (None, 128, 128)     0           ['layer_normalization_77[0][0]', \n",
            "                                                                  'add_55[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_46 (Dropout)           (None, 128, 128)     0           ['add_56[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_78 (LayerN  (None, 128, 128)    256         ['dropout_46[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_29 (Multi  (None, 128, 128)    66048       ['layer_normalization_78[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_78[0][0]'] \n",
            "                                                                                                  \n",
            " add_57 (Add)                   (None, 128, 128)     0           ['multi_head_attention_29[0][0]',\n",
            "                                                                  'dropout_46[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_79 (LayerN  (None, 128, 128)    256         ['add_57[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_41 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_79[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_47 (Dropout)           (None, 128, 128)     0           ['conv1d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_42 (Conv1D)             (None, 128, 128)     16512       ['dropout_47[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_80 (LayerN  (None, 128, 128)    256         ['conv1d_42[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_58 (Add)                   (None, 128, 128)     0           ['layer_normalization_80[0][0]', \n",
            "                                                                  'add_57[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_48 (Dropout)           (None, 128, 128)     0           ['add_58[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_81 (LayerN  (None, 128, 128)    256         ['dropout_48[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_30 (Multi  (None, 128, 128)    66048       ['layer_normalization_81[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_81[0][0]'] \n",
            "                                                                                                  \n",
            " add_59 (Add)                   (None, 128, 128)     0           ['multi_head_attention_30[0][0]',\n",
            "                                                                  'dropout_48[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_82 (LayerN  (None, 128, 128)    256         ['add_59[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_43 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_82[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_49 (Dropout)           (None, 128, 128)     0           ['conv1d_43[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_44 (Conv1D)             (None, 128, 128)     16512       ['dropout_49[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_83 (LayerN  (None, 128, 128)    256         ['conv1d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_60 (Add)                   (None, 128, 128)     0           ['layer_normalization_83[0][0]', \n",
            "                                                                  'add_59[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_50 (Dropout)           (None, 128, 128)     0           ['add_60[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_84 (LayerN  (None, 128, 128)    256         ['dropout_50[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_31 (Multi  (None, 128, 128)    66048       ['layer_normalization_84[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_84[0][0]'] \n",
            "                                                                                                  \n",
            " add_61 (Add)                   (None, 128, 128)     0           ['multi_head_attention_31[0][0]',\n",
            "                                                                  'dropout_50[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_85 (LayerN  (None, 128, 128)    256         ['add_61[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_45 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_85[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_51 (Dropout)           (None, 128, 128)     0           ['conv1d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_46 (Conv1D)             (None, 128, 128)     16512       ['dropout_51[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_86 (LayerN  (None, 128, 128)    256         ['conv1d_46[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_62 (Add)                   (None, 128, 128)     0           ['layer_normalization_86[0][0]', \n",
            "                                                                  'add_61[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_52 (Dropout)           (None, 128, 128)     0           ['add_62[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_87 (LayerN  (None, 128, 128)    256         ['dropout_52[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_32 (Multi  (None, 128, 128)    66048       ['layer_normalization_87[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_87[0][0]'] \n",
            "                                                                                                  \n",
            " add_63 (Add)                   (None, 128, 128)     0           ['multi_head_attention_32[0][0]',\n",
            "                                                                  'dropout_52[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_88 (LayerN  (None, 128, 128)    256         ['add_63[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_47 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_88[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_53 (Dropout)           (None, 128, 128)     0           ['conv1d_47[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_48 (Conv1D)             (None, 128, 128)     16512       ['dropout_53[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_89 (LayerN  (None, 128, 128)    256         ['conv1d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_64 (Add)                   (None, 128, 128)     0           ['layer_normalization_89[0][0]', \n",
            "                                                                  'add_63[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_54 (Dropout)           (None, 128, 128)     0           ['add_64[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_90 (LayerN  (None, 128, 128)    256         ['dropout_54[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_33 (Multi  (None, 128, 128)    66048       ['layer_normalization_90[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_90[0][0]'] \n",
            "                                                                                                  \n",
            " add_65 (Add)                   (None, 128, 128)     0           ['multi_head_attention_33[0][0]',\n",
            "                                                                  'dropout_54[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_91 (LayerN  (None, 128, 128)    256         ['add_65[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_49 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_91[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_55 (Dropout)           (None, 128, 128)     0           ['conv1d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_50 (Conv1D)             (None, 128, 128)     16512       ['dropout_55[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_92 (LayerN  (None, 128, 128)    256         ['conv1d_50[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_66 (Add)                   (None, 128, 128)     0           ['layer_normalization_92[0][0]', \n",
            "                                                                  'add_65[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_56 (Dropout)           (None, 128, 128)     0           ['add_66[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,011,840\n",
            "Trainable params: 1,011,840\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 20, 64, 64, 3)]   0         \n",
            "                                                                 \n",
            " model_4 (Functional)        (None, 128, 128)          1011840   \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirectio  (None, 128, 1024)        2625536   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_8 (Bidirectio  (None, 128, 1024)        6295552   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_9 (Bidirectio  (None, 128, 1024)        6295552   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_10 (Bidirecti  (None, 128, 1024)        6295552   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_11 (Bidirecti  (None, 128, 1024)        6295552   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_12 (Bidirecti  (None, 128, 1024)        6295552   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_13 (Bidirecti  (None, 128, 1024)        6295552   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 131072)            0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 4)                 524292    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41,934,980\n",
            "Trainable params: 41,934,980\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "trans_model = build_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5OfGYgMlk9-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJyNYBHpNKf5",
        "outputId": "ba1ecdc3-fa0f-4636-9858-c6c2452002aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_10 (InputLayer)          [(None, 20, 64, 64,  0           []                               \n",
            "                                 3)]                                                              \n",
            "                                                                                                  \n",
            " tubelet_embedding_6 (TubeletEm  (None, 128, 128)    196736      ['input_10[0][0]']               \n",
            " bedding)                                                                                         \n",
            "                                                                                                  \n",
            " positional_encoder_6 (Position  (None, 128, 128)    16384       ['tubelet_embedding_6[0][0]']    \n",
            " alEncoder)                                                                                       \n",
            "                                                                                                  \n",
            " layer_normalization_93 (LayerN  (None, 128, 128)    256         ['positional_encoder_6[0][0]']   \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_34 (Multi  (None, 128, 128)    66048       ['layer_normalization_93[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_93[0][0]'] \n",
            "                                                                                                  \n",
            " add_67 (Add)                   (None, 128, 128)     0           ['multi_head_attention_34[0][0]',\n",
            "                                                                  'positional_encoder_6[0][0]']   \n",
            "                                                                                                  \n",
            " layer_normalization_94 (LayerN  (None, 128, 128)    256         ['add_67[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_51 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_94[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_57 (Dropout)           (None, 128, 128)     0           ['conv1d_51[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_52 (Conv1D)             (None, 128, 128)     16512       ['dropout_57[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_95 (LayerN  (None, 128, 128)    256         ['conv1d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_68 (Add)                   (None, 128, 128)     0           ['layer_normalization_95[0][0]', \n",
            "                                                                  'add_67[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_58 (Dropout)           (None, 128, 128)     0           ['add_68[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_96 (LayerN  (None, 128, 128)    256         ['dropout_58[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_35 (Multi  (None, 128, 128)    66048       ['layer_normalization_96[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_96[0][0]'] \n",
            "                                                                                                  \n",
            " add_69 (Add)                   (None, 128, 128)     0           ['multi_head_attention_35[0][0]',\n",
            "                                                                  'dropout_58[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_97 (LayerN  (None, 128, 128)    256         ['add_69[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_53 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_97[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_59 (Dropout)           (None, 128, 128)     0           ['conv1d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_54 (Conv1D)             (None, 128, 128)     16512       ['dropout_59[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_98 (LayerN  (None, 128, 128)    256         ['conv1d_54[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_70 (Add)                   (None, 128, 128)     0           ['layer_normalization_98[0][0]', \n",
            "                                                                  'add_69[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_60 (Dropout)           (None, 128, 128)     0           ['add_70[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_99 (LayerN  (None, 128, 128)    256         ['dropout_60[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_36 (Multi  (None, 128, 128)    66048       ['layer_normalization_99[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_99[0][0]'] \n",
            "                                                                                                  \n",
            " add_71 (Add)                   (None, 128, 128)     0           ['multi_head_attention_36[0][0]',\n",
            "                                                                  'dropout_60[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_100 (Layer  (None, 128, 128)    256         ['add_71[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv1d_55 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_100[0][0]']\n",
            "                                                                                                  \n",
            " dropout_61 (Dropout)           (None, 128, 128)     0           ['conv1d_55[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_56 (Conv1D)             (None, 128, 128)     16512       ['dropout_61[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_101 (Layer  (None, 128, 128)    256         ['conv1d_56[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_72 (Add)                   (None, 128, 128)     0           ['layer_normalization_101[0][0]',\n",
            "                                                                  'add_71[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_62 (Dropout)           (None, 128, 128)     0           ['add_72[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_102 (Layer  (None, 128, 128)    256         ['dropout_62[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_37 (Multi  (None, 128, 128)    66048       ['layer_normalization_102[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_102[0][0]']\n",
            "                                                                                                  \n",
            " add_73 (Add)                   (None, 128, 128)     0           ['multi_head_attention_37[0][0]',\n",
            "                                                                  'dropout_62[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_103 (Layer  (None, 128, 128)    256         ['add_73[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv1d_57 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_103[0][0]']\n",
            "                                                                                                  \n",
            " dropout_63 (Dropout)           (None, 128, 128)     0           ['conv1d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_58 (Conv1D)             (None, 128, 128)     16512       ['dropout_63[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_104 (Layer  (None, 128, 128)    256         ['conv1d_58[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_74 (Add)                   (None, 128, 128)     0           ['layer_normalization_104[0][0]',\n",
            "                                                                  'add_73[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_64 (Dropout)           (None, 128, 128)     0           ['add_74[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_105 (Layer  (None, 128, 128)    256         ['dropout_64[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_38 (Multi  (None, 128, 128)    66048       ['layer_normalization_105[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_105[0][0]']\n",
            "                                                                                                  \n",
            " add_75 (Add)                   (None, 128, 128)     0           ['multi_head_attention_38[0][0]',\n",
            "                                                                  'dropout_64[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_106 (Layer  (None, 128, 128)    256         ['add_75[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv1d_59 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_106[0][0]']\n",
            "                                                                                                  \n",
            " dropout_65 (Dropout)           (None, 128, 128)     0           ['conv1d_59[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_60 (Conv1D)             (None, 128, 128)     16512       ['dropout_65[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_107 (Layer  (None, 128, 128)    256         ['conv1d_60[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_76 (Add)                   (None, 128, 128)     0           ['layer_normalization_107[0][0]',\n",
            "                                                                  'add_75[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_66 (Dropout)           (None, 128, 128)     0           ['add_76[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_108 (Layer  (None, 128, 128)    256         ['dropout_66[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_39 (Multi  (None, 128, 128)    66048       ['layer_normalization_108[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_108[0][0]']\n",
            "                                                                                                  \n",
            " add_77 (Add)                   (None, 128, 128)     0           ['multi_head_attention_39[0][0]',\n",
            "                                                                  'dropout_66[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_109 (Layer  (None, 128, 128)    256         ['add_77[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv1d_61 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_109[0][0]']\n",
            "                                                                                                  \n",
            " dropout_67 (Dropout)           (None, 128, 128)     0           ['conv1d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_62 (Conv1D)             (None, 128, 128)     16512       ['dropout_67[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_110 (Layer  (None, 128, 128)    256         ['conv1d_62[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_78 (Add)                   (None, 128, 128)     0           ['layer_normalization_110[0][0]',\n",
            "                                                                  'add_77[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_68 (Dropout)           (None, 128, 128)     0           ['add_78[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_111 (Layer  (None, 128, 128)    256         ['dropout_68[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_40 (Multi  (None, 128, 128)    66048       ['layer_normalization_111[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_111[0][0]']\n",
            "                                                                                                  \n",
            " add_79 (Add)                   (None, 128, 128)     0           ['multi_head_attention_40[0][0]',\n",
            "                                                                  'dropout_68[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_112 (Layer  (None, 128, 128)    256         ['add_79[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv1d_63 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_112[0][0]']\n",
            "                                                                                                  \n",
            " dropout_69 (Dropout)           (None, 128, 128)     0           ['conv1d_63[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_64 (Conv1D)             (None, 128, 128)     16512       ['dropout_69[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_113 (Layer  (None, 128, 128)    256         ['conv1d_64[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_80 (Add)                   (None, 128, 128)     0           ['layer_normalization_113[0][0]',\n",
            "                                                                  'add_79[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_70 (Dropout)           (None, 128, 128)     0           ['add_80[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_114 (Layer  (None, 128, 128)    256         ['dropout_70[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_41 (Multi  (None, 128, 128)    66048       ['layer_normalization_114[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_114[0][0]']\n",
            "                                                                                                  \n",
            " add_81 (Add)                   (None, 128, 128)     0           ['multi_head_attention_41[0][0]',\n",
            "                                                                  'dropout_70[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_115 (Layer  (None, 128, 128)    256         ['add_81[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv1d_65 (Conv1D)             (None, 128, 128)     16512       ['layer_normalization_115[0][0]']\n",
            "                                                                                                  \n",
            " dropout_71 (Dropout)           (None, 128, 128)     0           ['conv1d_65[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_66 (Conv1D)             (None, 128, 128)     16512       ['dropout_71[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_116 (Layer  (None, 128, 128)    256         ['conv1d_66[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_82 (Add)                   (None, 128, 128)     0           ['layer_normalization_116[0][0]',\n",
            "                                                                  'add_81[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_72 (Dropout)           (None, 128, 128)     0           ['add_82[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,011,840\n",
            "Trainable params: 1,011,840\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_9 (InputLayer)        [(None, 20, 64, 64, 3)]   0         \n",
            "                                                                 \n",
            " model_6 (Functional)        (None, 128, 128)          1011840   \n",
            "                                                                 \n",
            " bidirectional_14 (Bidirecti  (None, 128, 1024)        2625536   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_15 (Bidirecti  (None, 128, 1024)        6295552   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_16 (Bidirecti  (None, 128, 1024)        6295552   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_17 (Bidirecti  (None, 128, 1024)        6295552   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_18 (Bidirecti  (None, 128, 1024)        6295552   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_19 (Bidirecti  (None, 128, 1024)        6295552   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_20 (Bidirecti  (None, 128, 1024)        6295552   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 131072)            0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 4)                 524292    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41,934,980\n",
            "Trainable params: 41,934,980\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "11/11 [==============================] - 851s 73s/step - loss: 3.8089 - accuracy: 0.2805 - top-5-accuracy: 1.0000 - val_loss: 3.7261 - val_accuracy: 0.3514 - val_top-5-accuracy: 1.0000\n",
            "Epoch 2/60\n",
            "11/11 [==============================] - 773s 70s/step - loss: 3.6946 - accuracy: 0.3201 - top-5-accuracy: 1.0000 - val_loss: 4.3116 - val_accuracy: 0.2162 - val_top-5-accuracy: 1.0000\n",
            "Epoch 3/60\n",
            "11/11 [==============================] - 768s 70s/step - loss: 3.6169 - accuracy: 0.3780 - top-5-accuracy: 1.0000 - val_loss: 3.5061 - val_accuracy: 0.4865 - val_top-5-accuracy: 1.0000\n",
            "Epoch 4/60\n",
            "11/11 [==============================] - 766s 70s/step - loss: 3.4857 - accuracy: 0.4756 - top-5-accuracy: 1.0000 - val_loss: 3.4902 - val_accuracy: 0.4324 - val_top-5-accuracy: 1.0000\n",
            "Epoch 5/60\n",
            "11/11 [==============================] - 751s 68s/step - loss: 3.3787 - accuracy: 0.4939 - top-5-accuracy: 1.0000 - val_loss: 3.4471 - val_accuracy: 0.4595 - val_top-5-accuracy: 1.0000\n",
            "Epoch 6/60\n",
            "11/11 [==============================] - 755s 69s/step - loss: 3.3393 - accuracy: 0.4482 - top-5-accuracy: 1.0000 - val_loss: 3.2851 - val_accuracy: 0.6216 - val_top-5-accuracy: 1.0000\n",
            "Epoch 7/60\n",
            "11/11 [==============================] - 757s 69s/step - loss: 3.3221 - accuracy: 0.4573 - top-5-accuracy: 1.0000 - val_loss: 3.2768 - val_accuracy: 0.5405 - val_top-5-accuracy: 1.0000\n",
            "Epoch 8/60\n",
            "11/11 [==============================] - 749s 68s/step - loss: 3.2002 - accuracy: 0.4878 - top-5-accuracy: 1.0000 - val_loss: 3.2156 - val_accuracy: 0.4865 - val_top-5-accuracy: 1.0000\n",
            "Epoch 9/60\n",
            "11/11 [==============================] - 754s 68s/step - loss: 3.1220 - accuracy: 0.5518 - top-5-accuracy: 1.0000 - val_loss: 3.5222 - val_accuracy: 0.3514 - val_top-5-accuracy: 1.0000\n",
            "Epoch 10/60\n",
            "11/11 [==============================] - 750s 68s/step - loss: 3.0617 - accuracy: 0.5213 - top-5-accuracy: 1.0000 - val_loss: 3.1940 - val_accuracy: 0.5405 - val_top-5-accuracy: 1.0000\n",
            "Epoch 11/60\n",
            "11/11 [==============================] - 755s 68s/step - loss: 3.0341 - accuracy: 0.5366 - top-5-accuracy: 1.0000 - val_loss: 3.3855 - val_accuracy: 0.4324 - val_top-5-accuracy: 1.0000\n",
            "Epoch 12/60\n",
            "11/11 [==============================] - 750s 68s/step - loss: 2.9115 - accuracy: 0.5762 - top-5-accuracy: 1.0000 - val_loss: 3.0668 - val_accuracy: 0.4865 - val_top-5-accuracy: 1.0000\n",
            "Epoch 13/60\n",
            "11/11 [==============================] - 755s 69s/step - loss: 2.8227 - accuracy: 0.6098 - top-5-accuracy: 1.0000 - val_loss: 3.1554 - val_accuracy: 0.4865 - val_top-5-accuracy: 1.0000\n",
            "Epoch 14/60\n",
            "11/11 [==============================] - 759s 69s/step - loss: 2.7921 - accuracy: 0.5854 - top-5-accuracy: 1.0000 - val_loss: 3.2761 - val_accuracy: 0.4054 - val_top-5-accuracy: 1.0000\n",
            "Epoch 15/60\n",
            "11/11 [==============================] - 751s 68s/step - loss: 2.6856 - accuracy: 0.6311 - top-5-accuracy: 1.0000 - val_loss: 3.0866 - val_accuracy: 0.4595 - val_top-5-accuracy: 1.0000\n",
            "Epoch 16/60\n",
            "11/11 [==============================] - 773s 70s/step - loss: 2.7623 - accuracy: 0.5732 - top-5-accuracy: 1.0000 - val_loss: 3.0131 - val_accuracy: 0.4324 - val_top-5-accuracy: 1.0000\n",
            "Epoch 17/60\n",
            " 5/11 [============>.................] - ETA: 7:06 - loss: 2.6783 - accuracy: 0.5813 - top-5-accuracy: 1.0000"
          ]
        }
      ],
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "    \"\"\"\n",
        "    However, you mentioned earlier that you were using the SparseCategoricalCrossentropy loss function, which expects integer labels rather than one-hot-encoded vectors. If you're using one-hot-encoded labels, you should switch to the CategoricalCrossentropy loss function instead.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "        metrics=[\n",
        "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = '/content/drive/MyDrive/poseestimization/lastm/vitbidirection_model_weights.h5'\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=features_train,\n",
        "        y=labels_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(features_test,labels_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "vit_classifier = build_model()\n",
        "history = run_experiment(vit_classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-G7dN4NPZYP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}